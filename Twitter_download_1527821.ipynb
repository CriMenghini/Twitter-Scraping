{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Algorithmic methods for data mining\n",
    "  # Retrieving data from Twitter\n",
    "        \n",
    "Cristina Menghini - StudentID: 1527821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task of the project is to build a graph that describese the network of users who tweetted about a certain specific query. To do that I achieved the following step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creation of APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created an API that allowed me to retrieve data from twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = { \n",
    "    \"consumer_key\"        : \"FIdkHUbZuNjNxyPfImqI9TFXN\",\n",
    "    \"consumer_secret\"     : \"lVyiiX67XiU3EkvsrC20yj2pDPhsQvUNFGr2FuGdvdmJxd1vwM\",\n",
    "    \"access_token\"        : \"633193657-arMpLiTtLAlAGDfkUUqcPnx4obmgrE7VkTRUgJmO\",\n",
    "    \"access_token_secret\" : \"2TLsxc9FUYUYVpeaTkPGkuL7Nmk5PwQQv5q9JbdrACZh8\" \n",
    "}\n",
    "\n",
    "auth = OAuthHandler(cfg[\"consumer_key\"], cfg[\"consumer_secret\"])\n",
    "auth.set_access_token(cfg[\"access_token\"], cfg[\"access_token_secret\"]) \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the retrieving operations I used many different APIs each of whose access key is stored into a file. That files that contain all the APIs keys are: 'consumer_key.txt', 'secret_consumer.txt', 'access_token.txt', 'secret_token.txt'. I decide to save the keys in external files because seeing the in the code makes it messy and, of course, also to not risk to lose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListKeys(doc_key):\n",
    "    \"\"\"This function creates the list of the keys of all APIs I'm going to use. \n",
    "    - doc_key: the file in which the keys are saved.\"\"\"\n",
    "    list_keys = [line for line in open(doc_key, 'r+')]\n",
    "    keys = [list_keys[i].rstrip() for i in range(len(list_keys))]\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I define the lists of the different keys. \n",
    "cons_key = ListKeys('consumer_key.txt')\n",
    "cons_sec = ListKeys('secret_consumer.txt')\n",
    "acc_tok = ListKeys('access_token.txt')\n",
    "acc_tok_sec = ListKeys('secret_token.txt')\n",
    "# Then I zip all of them in a unique variable.\n",
    "input_crate_api = zip(cons_key, cons_sec, acc_tok, acc_tok_sec) # The len of that variable corresponds to the number of applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Create_API(var_input, a):\n",
    "    \"\"\"This function define a API using specific keys. Its return is the API.\n",
    "    - var_input is the tuple of keys;\n",
    "    - a is the app i refer to.\"\"\"\n",
    "    cfg = { \n",
    "    \"consumer_key\"        : var_input[a][0],\n",
    "    \"consumer_secret\"     : var_input[a][1],\n",
    "    \"access_token\"        : var_input[a][2],\n",
    "    \"access_token_secret\" : var_input[a][3]\n",
    "    }\n",
    "    auth = OAuthHandler(cfg[\"consumer_key\"], cfg[\"consumer_secret\"])\n",
    "    auth.set_access_token(cfg[\"access_token\"], cfg[\"access_token_secret\"]) \n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the data download, I define a database in MongoDB ( based on 'localhost'). At first I built the db named 'Project' and then I am going to add collections step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "# Define the client of the db\n",
    "client = MongoClient('localhost', 27017)\n",
    "# Create the database\n",
    "project_2 = client['Project']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that point we are able to retrieve informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and collect tweets that contains a particular query. Suppose the topic is 'hello', each tweet will be saved as a dictionary:\n",
    "\n",
    "                example_one_tweet = {'user_id' : '17','tweet' : 'hello world, Finally here I am!' }\n",
    "    \n",
    "The dictionaries will be filled in the specific 'user_tweet' collection of Project db. The user ids will be also stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the collection of users and their tweets\n",
    "collection = project_2['Collection_Project']\n",
    "us_tw = project_2.user_tweet # It's named 'user_tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Get_Users_and_Tweet(topic, num_tweet, max_tweet):\n",
    "    \"\"\"This function download the tweet and the id of the user who tweets, it also stores the data in user and tweet\n",
    "    collection of MongoDB and gives back the list of users id.\n",
    "    - num_tweet is the number of tweet to download per page;\n",
    "    - max_tweet is the numer of total tweets that I want to collect.\"\"\"\n",
    "    tweet_count = 0  \n",
    "    user_ids = [] # Inizialization of user_ids list\n",
    "    while True:\n",
    "        try:          \n",
    "            for c in tweepy.Cursor(api.search, q = topic , count = num_tweet).pages(): # Scroll the cursor\n",
    "                tweet_count += len(c) # Adjourn the number of collected tweets\n",
    "                print tweet_count , 'tweets already collected.'\n",
    "                us_tw.insert_many([{'user_id' : str(i.author.id) , 'tweet' : i.text} for i in c])# Insert elements in the collection\n",
    "                user_ids += [i.author.id for i in c] \n",
    "                if tweet_count >= max_tweet : # Limit of tweets to download\n",
    "                    break\n",
    "        except tweepy.TweepError:\n",
    "            print 'Requests excedeed. Sleep for 15 minutes.'\n",
    "            time.sleep(15*60+10)\n",
    "            continue # Continue to scroll the pages\n",
    "        break \n",
    "    return user_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following lines of code I use the function define above to retrieve tweets and users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_user = Get_Users_and_Tweet('yearofmercy', 100, 10000)\n",
    "users = set(ids_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to save some data also into files because it helps me to do more efficent operations( considerig the large scale of data) in the memory avoidig the research on the disk of the elements contained in local MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( users, open('users.p', 'wb')) # Store the variable users in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now I collected the users and their tweets. By now I'm interesting in collecting the followers of each user. \n",
    "\n",
    "In order to study the network of users, I want to collect only the followers that are already in the list of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collect followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of retrieving followers consists in download data and store them in 'user_followers' collection MongoDB. Each element will be:\n",
    "\n",
    "                        example_one_follower = {'user_id' : '17','follower' : '21'}\n",
    "                        \n",
    "This will be an entry of the collection and each of this element will be also saved into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Get_followers(us, API, thre):\n",
    "    \"\"\"This function downloads the followers of the users list and store them in Followers Mongodb collection.\n",
    "    - us is the list of users id;\n",
    "    - API is the api I use to download data;\n",
    "    - num_page number of elements to consider in a page;\n",
    "    - thre is the number of thread that's working.\"\"\"  \n",
    "    global list_followers\n",
    "    for u in us:\n",
    "        c = tweepy.Cursor(API.followers_ids, user_id = u, count = 5000).pages() # Define the cursor\n",
    "        foll_user = []  # Define the list that will contain the lists of collected followers of user u for each page.        \n",
    "        while True:                             \n",
    "            try:\n",
    "                foll_page = c.next() # Obtain the list of followers related to the first page\n",
    "                foll_user += foll_page # Add the elements of the followers of that page to the list of the all followers of u  \n",
    "            except tweepy.TweepError:\n",
    "                print 'An error occurs. Sleep for 15 minutes. Thread:', thre\n",
    "                e = threading.Event()\n",
    "                e.wait(timeout = 15*60+10)\n",
    "            except StopIteration: # When the list id u followers ends.\n",
    "                break    \n",
    "        followers = foll_user\n",
    "        if len(followers) > 0 : # Check whether the list is empty, if it is not.\n",
    "                fol_list = [{'user_id' : str(u), 'follower' : str(follower)} for follower in followers if str(follower) in users]# Create the list of edges btw users and followers that belong in the users list\n",
    "                if len(fol_list) > 0: # To insert the list in Mongo, assure that it's length is grater than 0.\n",
    "                    us_fol.insert_many(fol_list)#Here I add to Mongo collection the list of element each of which correspond to an edge of the graph.\n",
    "                    list_followers += fol_list # Add elements to the list of all edges.\n",
    "        print us_fol.count()\n",
    "    print 'All followers collected.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the large number of users that need to be processed I decide to use the multi-threading process that allows me to compute the operations in less time.\n",
    "\n",
    "Therefore i split the list of users into n lists that correspond to the number of thread I'll run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def users_chunks(n, users_list):\n",
    "    \"\"\"This function returns the list of chunks that will be used during the multi-threading.\n",
    "    - n is the number of threads;\n",
    "    - user_list is the list of user to split.\"\"\"\n",
    "    num = float(len(users_list))/n \n",
    "    us_lists = [ users_list [i:i + int(num)] for i in range(0, (n-1)*int(num), int(num))]\n",
    "    us_lists.append(users_list[(n-1)*int(num):])\n",
    "    return us_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before running the multi-threading process I define the function that will be applied by each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def worker(us_ID, Api , THREAD):\n",
    "    \"\"\"This function define the work that each thread is going to do.\n",
    "    The variables of the function are the arguments of Get_followers function.\"\"\"\n",
    "    Get_followers(us_ID, Api, THREAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, I define the new collection where will be stored the edges of the graph that will represent the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145267"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection 'user_followers'\n",
    "us_fol = project_2.user_followers\n",
    "us_fol.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the list of users into smaller lists\n",
    "user_lists = users_chunks(26, list(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the entire list of edges\n",
    "list_followers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the followers\n",
    "threads = []\n",
    "for j in range(len(user_lists)):\n",
    "    t = threading.Thread(target = worker, args = (user_lists[j], Create_API(input_crate_api, j), j)) # That's the funcion that each thread will execute\n",
    "    threads.append(t)\n",
    "    time.sleep(5)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps, to make the operations faster I save the edges also in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the variable List_followers in a txt file\n",
    "pickle.dump( list_followers, open( \"save.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
